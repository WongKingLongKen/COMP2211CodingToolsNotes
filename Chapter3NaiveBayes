labelled dataset def: labels (inputs with correct outputs) inside training dataset 

supervised learning: regression & classification -> learn the mapping
unsupervised learning: clustering & association -> learn themselves & find a pattern in unlabelled dataset
* association def: a rule-based machine learning to discover the probability of the co-occurrence of items in a set

P(B|E) = P(B)xP(E|B)/P(E)
Posterior Prob = Prior Prob x Likelihood / Marginal Prob

As mentioned, we can simplify Bayes’ rule by making these two simple assumptions:
The features (or Evidence) related to a belief are independent from one another.
Each piece of evidence has the same importance in determining the outcome.
 
Remember, the Naïve Bayes rule is defined as follows.
 
P(Bi | E) = P(Bi) P(e1|Bi) P(e2|Bi) P(e3|Bi) … P(ed|Bi) / [ ( P(e1|B1) P(e2|B1)P(e3|B1)… P(ed|B1)P(B1) ) + … + ( P(e1|Bn) P(e2|Bn)P(e3|Bn)… P(ed|Bn)P(Bn) ) ]
Remark: We have n pieces of belief, and d pieces of evidence
 
Referring to the Problem of suffering from disease Z, if you apply the Naïve Bayes rule, we get the following.
 
Question: Estimate if a person has disease Z based on their symptoms
(Blood Pressure = high, Fever = no, Diabetes = yes, and Vomit = yes)
Solution:
Compute P( Disease Z = Yes | (Blood Pressure = high, Fever = no, Diabetes = yes, and Vomit = yes) )
P( Disease Z = Yes ) P( Blood Pressure = high | Disease Z = Yes ) P( Fever = no | Disease Z = Yes ) P( Diabetes = yes | Disease Z = Yes ) P( Vomit = yes | Disease Z = Yes ) /
[ ( P( Disease Z = Yes ) P( Blood Pressure = high | Disease Z = Yes ) P( Fever = no | Disease Z = Yes ) P( Diabetes = yes | Disease Z = Yes ) P( Vomit = yes | Disease Z = Yes ) ) +
  ( P( Disease Z = No ) P( Blood Pressure = high | Disease Z = No ) P( Fever = no | Disease Z = No ) P( Diabetes = yes | Disease Z = No ) P( Vomit = yes | Disease Z = No )  ) ]
 
Compute P( Disease Z = No | (Blood Pressure = high, Fever = no, Diabetes = yes, and Vomit = yes) )
P( Disease Z = No ) P( Blood Pressure = high | Disease Z = No ) P( Fever = no | Disease Z = No ) P( Diabetes = yes | Disease Z = No ) P( Vomit = yes | Disease Z = No ) /
[ ( P( Disease Z = Yes ) P( Blood Pressure = high | Disease Z = Yes ) P( Fever = no | Disease Z = Yes ) P( Diabetes = yes | Disease Z = Yes ) P( Vomit = yes | Disease Z = Yes ) ) +
  ( P( Disease Z = No ) P( Blood Pressure = high | Disease Z = No ) P( Fever = no | Disease Z = No ) P( Diabetes = yes | Disease Z = No ) P( Vomit = yes | Disease Z = No )  ) ]
 
As you can see, the denominators (shown in red squared brackets) for P( Disease Z = Yes | (Blood Pressure = high, Fever = no, Diabetes = yes, and Vomit = yes) ) and P( Disease Z = No | (Blood Pressure = high, Fever = no, Diabetes = yes, and Vomit = yes) ), are the same. Therefore, to determine if the person has disease Z, we only need to look at the numerators and see which one is larger, i.e.
 
If P( Disease Z = Yes ) P( Blood Pressure = high | Disease Z = Yes ) P( Fever = no | Disease Z = Yes ) P( Diabetes = yes | Disease Z = Yes ) P( Vomit = yes | Disease Z = Yes ) IS GREATER THAN P( Disease Z = No ) P( Blood Pressure = high | Disease Z = No ) P( Fever = no | Disease Z = No ) P( Diabetes = yes | Disease Z = No ) P( Vomit = yes | Disease Z = No ), then the person is likely suffering from disease Z.
Otherwise, the person is unlikely suffering from disease Z.

we are usually asked to predict the value of something using Naive Bayes by P(B_i|E) = P(E|B_i)P(B_i)/P(E) = P(B_i)[P(e1|B_i)...(en|B_i)]
for example, P(Yes|e1,e2,e3,e4) = P(Yes)P(e1|Yes)P(e2|Yes)P(e3|Yes)P(e4|Yes)
P(No|e1,e2,e3,e4) = P(No)P(e1|No)P(e2|No)P(e3|No)P(e4|No)
Compare which prob is larger and do normalization (optional or required to do so)

When we want to know which belief has the highest probability rather than the whole fraction:
Remove the denominator as B_NB = argmax_B_i P(B_i)(P(e1|B_i)P(e2|B_i)...P(ed|B_i))

Na ̈ıve Bayes can handle a dataset with missing value. Attributes are handled separately by the algorithm at both model construction time 
and prediction time. Therefore, the missing attributes can simply be ignored while preparing the model, and also ignored when a
probability is calculated for a class value.

Zero frequency def: unable to make a prediction when there's a category in the test dataset that isn't observed in the
training dataset and is assigned with a 0 probability. We will get zero when the probabilities related to the categories are multiplied together
In lab2, we can't use np.unique for a task becuz we'd ignore the category that doesn't show up in the training dataset,
i.e. we should count the category as 0

Solution to zero-frequency: alpha-Laplacian Smoothing
Add +1 to every feature-class count (even unseen combinations). Ensures no zero probabilities
P(ej=v_j|B_i=c_i) = [(Count of ej=v_j AND B_i=c_i) + alpha] / [(Count of B_i=c_i) + n*alpha]
in scikit-learn, we can't set alpha as 0 becuz it's by default 0

Underflow prevention: sometimes multiplying many values from 0 to 1 results in floating point underflow
solution: take log, B_NB = argmax_B_i (logP(B_i) + summation from n=1 to d logP(en|B_i))

application of Naive Bayes:
1. real time prediction
2. multiclass prediction
3. text classification, e.g. spam filtering and sentiment analysis
4. recommendation system

Naive Bayes
pros: easy to implement, high efficiency in computation, predict multi-classes, work well in large size dataset and NLP text classification
cons: strong assumption about the features to be independent that is not generally correct in real life, decreased precision value if the dataset size is small,
biased results if we prefer prediction of probability over classification, zero frequency issue

Naive Bayes computes the posterior probability of a class label given input features (e.g., classifying emails as "spam" or "not spam"). It cannot output continuous values 
required for regression (e.g., predicting temperature or house prices).
Regression Task:
Predict the amount of rainfall (in mm).

Naive Bayes cannot provide a continuous value like 
12.5mm; it would require discretizing rainfall into categories (e.g., "light," "heavy"), which reduces precision.

Why P(ei|B_i) can be found by plugging in x=ei to f(x)?
Prob = Density * Neighborhood Size
For a continuous r.v. with density f(x), P(x in B_i) = f(ei) * |B_i| where |B_i| is e.g. length, area of B_i
Why this approximation fails sometimes?
1. large B_i, leading to varying f(x)
2. non-uniform density, f(x) should be roughly constant over B_i
