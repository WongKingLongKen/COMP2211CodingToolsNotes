MLP def: output of the perceptron connected to the input of other perceptrons. As a kind of feed-forward neural network
MLP structure as follows:
![alt text](https://github.com/user-attachments/assets/db40c6ba-e4ab-4f05-b64b-b3a959fed1ef)

MLP procedure for tuning its accuracy:
1. initialize small random values for weights (w) and bias (theta)
2. input something to node i (input layer) and calculate the output by the network with the given inputs, 
remarks: inputs and outputs of input layer also called inputs (forward propagation)
3. pass through activation functions such as sigmoid functions, to node j (hidden layer), until node k (output layer)
4. check the error (loss function) by Actual Output - Target Output
5. update the weights and bias between node j and k (backward propagation)
6. update the weights and bias between node i and j (b.p.)
7. begin from step 2 again

When to stop training?
- once the training error falls below some thresholds
- after a fixed number of iterations via the loop
- reach the minimum error of the validation set

Gradient Descent def: we aim at going in the direction of the deepest descent and minimize the loss function by finding the local minimum
or global minimum

Pros: 
1. No need to find closed form math solution
2. Computationally faster and cheaper

It's wrong to say that increasing the number of hidden layers always increases the model performance
since it may cause overfitting, vanishing gradients, computational complexity, leading to worse generalization.

Why we shouldn't initialize all the weights of an MLP as zero?
Poor performance. 
- all the neurons propagate with the same gradient and biases
- different neurons learn the same feature

learning rate of an MLP issue:
too large.) the training will be inconsistent and unstable such that the model may cover too quickly to a sub-optimal solution
too small.) learn too slowly

ReLu: range [0, inf)
Most state-of-the-art models use rectified linear units (ReLU) as non-linearity instead of Sigmoid function in a deep neural network. The question is why?
	1	Non-Linearity: ReLU introduces non-linearity, enabling the network to learn complex patterns without saturating for positive inputs.
	2	Gradient Efficiency: Unlike sigmoid, ReLU avoids the vanishing gradient problem, allowing efficient backpropagation in deep networks.
	3	Computational Simplicity: ReLU is computationally efficient due to its simple thresholding operation.

VS

sigmoid: 
The sigmoid function maps input values to a value between 0 and 1, making it useful for binary classification and logistic regression problems.

VS

Softmax:
finds its application in multi-class scenarios, where multiple classes are involved, assigning probabilities to each class for 
comprehensive classification. On the other hand, Sigmoid is tailored for binary classification tasks, focusing on distinguishing between 
two exclusive outcomes with probability mapping.

usually in hidden layers: we use kernel_regularizer to penalize (reduce) the weights that are large causing the model to overfit
e.g. model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.002)))
The key difference between L1 (lasso) and L2 (ridge) regularization techniques is that lasso regression shrinks the less important feature’s coefficient 
to zero, removing some features altogether. In other words, L1 regularization works well for feature selection in case we have a huge number of features.
Traditional methods like cross-validation and stepwise regression to perform feature selection and handle overfitting work well with a small set 
of features, but L1 and L2 regularization methods are a great alternative when you’re dealing with a large set of features.

tensorboard: use it when the weights are not updated due to vanishing or exploding gradients
a visualization tool that enables us to track metrics like loss and accuracy, visualze the model graph

