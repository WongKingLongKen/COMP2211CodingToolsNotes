Every pixel in an image is a feature/ input node

Numbers in the kernels are the weights connecting the feature of the input image and the node of the hidden layer
Each kernel has a corresponding bias term, so the number of biases is equal to the number of kernels.

input depth = no. of filter kernels
Each filter kernel in CNN is designed to detect specific patterns, e.g. edges, shapes. A vertical edge detector kernel will only respond strongly to vertical edges. 
If use more kernels, each one can specialise in detecting a different feature, increasing the diversity of extracted patterns. Early layers use kernels to detect 
low level features like edges or corners, later layers combine the kernels with low level features into higher level features like shapes or objects.

more sets of kernels put in, more types of features generated

each kernel can have different values since we fine tune them independently (but in fact we dun know the value of the kernels beforehand until CNN)

But implement too many kernels not good:
each additional kernel requires more floating point operations, taking longer to train and use more GPU/ CPU memory

Extremely deep/ wide network may cause:
- vanishing gradient (slow learning rate in early layers)
- exploding gradient (unstable training)

feature map - convolution output

Fine-tuning more parameters in a neural network becomes necessary in scenarios where the model's architecture is modified or scaled to handle increased complexity, larger data dimensions, or more nuanced feature extraction.

---

### **When Do We Need to Fine-Tune More Parameters?**

| **Factor**                  | **Explanation**                                                                 | **Impact on Parameters**                          | **Effect on Fine-Tuning Time**               |
|------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------|----------------------------------------------|
| **More Sets of Kernels**      | Adding more filters (kernels) to convolutional layers.                         | Increases parameters linearly with the number of filters. Each filter has its own weights. | Longer training due to more gradients to compute. |
| **Bigger Kernel Size**        | Using larger spatial dimensions for kernels (e.g., 5×5 instead of 3×3).        | Increases parameters quadratically (e.g., 25 vs. 9 weights per filter). | Larger kernels require more computations per layer. |
| **More Input Channels**       | Expanding input depth (e.g., RGB + depth = 4 channels instead of 3).           | Increases parameters in the first convolutional layer (kernels must match input channels). | Proportional increase in parameters and training time. |
| **Bigger Input Image Size**   | Processing higher-resolution images (e.g., 512×512 instead of 224×224).         | Only affects parameters if the model uses fully connected (FC) layers. FC layers tied to input size (rare in modern architectures). | Minimal impact if the network is fully convolutional (common in CNNs). |

---

### **Key Scenarios Requiring More Fine-Tuning**

1. **Complex Feature Learning**  
   - **Example**: Object detection in high-resolution medical images.  
   - **Why**: Larger kernel sizes or more filters help capture fine-grained details (e.g., tumor boundaries).  
   - **Trade-off**: Increased parameters improve accuracy but require longer training.

2. **Multi-Modal Inputs**  
   - **Example**: Combining RGB, infrared, and depth channels for autonomous driving.  
   - **Why**: Additional input channels require wider kernels in the first layer, increasing parameters.  

3. **Architectural Scaling**  
   - **Example**: Expanding a pre-trained ResNet-18 to ResNet-50 for a specialized task.  
   - **Why**: Deeper networks with more layers/filters inherently have more parameters to fine-tune.  

4. **Handling High-Resolution Data**  
   - **Example**: Video processing with 4K frames.  
   - **Why**: If the model includes FC layers, input size directly impacts parameter count.  

---

### **Mitigation Strategies for Efficiency**
- **Transfer Learning**: Start with pre-trained weights to reduce training time.  
- **Global Average Pooling**: Replace FC layers with pooling to decouple parameter count from input size.  
- **Depthwise Separable Convolutions**: Reduce parameters while maintaining performance (e.g., MobileNet).  

---

Strides:
larger strides lead to lesser overlaps that means lower output volume. 
lesser memory needed for output.
avoids overfitting.

CNN VS MLP:
Flattening in CNNs: While CNNs do include a flattening step, it typically occurs towards the end of the network, just before the fully connected
(dense) layers. By this point, the convolutional and pooling layers have already extracted and condensed the most important spatial features from 
the input data. The flattening step is used to transition from the convolutional layers to the dense layers, which are used for classification or 
regression tasks.

Flattening in MLPs: In an MLP, the input data is flattened right at the beginning. This means that an image, which is inherently a 2D or 3D structure,
is converted into a 1D vector. This flattening process loses the spatial relationships between pixels, making it harder for the network to learn 
patterns that depend on the spatial structure of the data.

Spatial Feature Preservation: CNNs preserve spatial relationships through convolutional layers, making them more effective at capturing local 
patterns and hierarchies in the data.
Parameter Efficiency: CNNs are more parameter-efficient compared to MLPs for image tasks. The use of kernels in convolutional layers reduces the 
number of parameters needed compared to fully connected layers in MLPs.
Hierarchical Learning: CNNs learn features in a hierarchical manner, starting from simple features like edges to more complex patterns, which is 
well-suited for image data.

ReLU is more popular for CNN as it doesn't require any expensive computation and has been shown to speed up the convergence of stochastic gradient
descent algorithms.

Pooling keeps the key information in each feature map thereby reducing noise and aiding in curtailing the risk of overfitting. Its a form of feature
selection. Computational Efficiency — Reducing dimensionality means fewer operations, rendering the result more computationally efficient.

Fix underfitting: Train the model for more epochs to allow it sufficient time to learn the data patterns.
